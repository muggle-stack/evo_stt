#!/usr/bin/env python3
"""
ASR 流式识别示例 - 边录边识别 (VAD + flush)

使用 multiprocessing 避免 GIL 导致的 overrun 问题。
音频采集在独立进程中运行，通过 multiprocessing.Queue 传递数据。

依赖:
    1. evo_asr 模块 (stt/build/python)
    2. evo_audio 模块 (audio/python)

编译 audio 组件:
    cd audio && mkdir -p build && cd build
    cmake .. && make -j$(nproc)

Usage:
    python stream_demo.py                 # 默认设备，持续识别
    python stream_demo.py -l              # 列出音频设备
    python stream_demo.py -d 0            # 使用设备 0
    python stream_demo.py --duration 10   # 录制 10 秒
    python stream_demo.py --vad-threshold 100000  # 调整 VAD 阈值
"""

import sys
import time
import argparse
import multiprocessing as mp
from multiprocessing import Process, Queue
import numpy as np

# 导入 evo_asr
try:
    import evo_asr
    from evo_asr import AsrCallback
except ImportError as e:
    print("错误: 无法导入 evo_asr 模块")
    print("请确保已设置 PYTHONPATH:")
    print("  export PYTHONPATH=/path/to/stt/build/python:/path/to/stt/python")
    print(f"\n详细错误: {e}")
    sys.exit(1)


# =============================================================================
# 音频处理工具
# =============================================================================

def resample_audio(audio: np.ndarray, src_rate: int, dst_rate: int) -> np.ndarray:
    """重采样音频 (简单线性下采样，适合实时处理)"""
    if src_rate == dst_rate:
        return audio

    # 使用简单的线性插值，速度快适合实时
    ratio = dst_rate / src_rate
    new_len = int(len(audio) * ratio)
    indices = np.linspace(0, len(audio) - 1, new_len)
    return np.interp(indices, np.arange(len(audio)), audio).astype(np.float32)


def stereo_to_mono(audio: np.ndarray, channels: int) -> np.ndarray:
    """多声道混音为单声道"""
    if channels == 1:
        return audio
    # 假设交错格式: L R L R ...
    audio = audio.reshape(-1, channels)
    return audio.mean(axis=1).astype(np.float32)


# =============================================================================
# 简单能量 VAD
# =============================================================================

class SimpleVAD:
    """
    简单的能量 VAD (Voice Activity Detection)

    检测静音来判断句子边界。当连续静音帧数超过阈值时，认为句子结束。
    """

    def __init__(self, threshold: float = 100000.0, min_silence_frames: int = 8,
                 min_speech_frames: int = 3):
        """
        Args:
            threshold: 静音能量阈值 (int16 平方均值)
            min_silence_frames: 最小静音帧数，超过则认为句子结束
            min_speech_frames: 最小语音帧数，需要先说话才能触发句子结束
        """
        self.threshold = threshold
        self.min_silence_frames = min_silence_frames
        self.min_speech_frames = min_speech_frames
        self.silence_frames = 0
        self.speech_frames = 0
        self.has_speech = False

    def feed(self, audio: np.ndarray) -> None:
        """输入音频帧 (int16 或 float32)"""
        if audio.dtype == np.float32:
            # 转换为 int16 范围计算能量
            audio_int = (audio * 32768).astype(np.float32)
        else:
            audio_int = audio.astype(np.float32)

        energy = np.mean(audio_int ** 2)

        if energy >= self.threshold:
            # 有语音
            self.speech_frames += 1
            self.silence_frames = 0
            if self.speech_frames >= self.min_speech_frames:
                self.has_speech = True
        else:
            # 静音
            self.silence_frames += 1

    def is_sentence_end(self) -> bool:
        """检测是否句子结束"""
        return self.has_speech and self.silence_frames >= self.min_silence_frames

    def reset(self) -> None:
        """重置状态"""
        self.silence_frames = 0
        self.speech_frames = 0
        self.has_speech = False


# =============================================================================
# 自定义回调类
# =============================================================================

class StreamingCallback(AsrCallback):
    """流式识别回调 - 实时打印识别结果"""

    def __init__(self):
        super().__init__()
        self.final_results = []
        self.sentence_count = 0

    def on_open(self) -> None:
        print(">>> [回调] 识别会话开始")

    def on_event(self, result) -> None:
        # 获取文本
        text = getattr(result, 'text', '') or result.get_text() if hasattr(result, 'get_text') else str(result)

        if text:
            self.sentence_count += 1
            print(f"[句子 {self.sentence_count}]: {text}")
            self.final_results.append(text)

    def on_complete(self) -> None:
        print("\n>>> [回调] 识别任务完成")

    def on_error(self, result) -> None:
        msg = getattr(result, 'message', str(result))
        print(f"\n>>> [回调] 错误: {msg}")

    def on_close(self) -> None:
        print(">>> [回调] 会话关闭")


def list_devices():
    """列出音频设备"""
    try:
        import evo_audio  # noqa: F401
        from evo_audio import AudioCapture
    except ImportError as e:
        print("错误: 无法导入 evo_audio 模块")
        print(f"详细错误: {e}")
        return

    print("=== 音频输入设备 ===")
    for idx, name in AudioCapture.list_devices():
        print(f"  [{idx}] {name}")


# =============================================================================
# 音频采集进程
# =============================================================================

def audio_capture_process(audio_queue: Queue, stop_event, device: int,
                          duration: float, config_queue: Queue):
    """
    独立进程中运行音频采集

    Args:
        audio_queue: 音频数据队列 (输出)
        stop_event: 停止信号
        device: 音频设备索引
        duration: 录音时长
        config_queue: 配置信息队列 (输出采样率和声道数)
    """
    try:
        import evo_audio
        from evo_audio import AudioCapture
    except ImportError as e:
        config_queue.put({'error': str(e)})
        return

    try:
        # 初始化音频
        evo_audio.init(
            sample_rate=16000,
            channels=1,
            chunk_size=3200,  # 100ms @ 16kHz
            capture_device=device,
        )

        config = evo_audio.get_config()
        input_rate = config['sample_rate']
        input_channels = config['channels']

        # 发送配置信息给主进程
        config_queue.put({
            'sample_rate': input_rate,
            'channels': input_channels,
        })

        frame_count = 0

        def on_audio(data: bytes):
            """音频回调 - 直接放入队列，无 GIL 竞争"""
            nonlocal frame_count
            frame_count += 1
            try:
                # 使用 put_nowait 避免阻塞
                audio_queue.put_nowait(data)
            except Exception:
                pass  # 队列满时丢弃

        with AudioCapture() as cap:
            cap.set_callback(on_audio)
            cap.start()

            print(f"[采集进程] 开始录音 ({duration}秒)...")

            start_time = time.time()
            while time.time() - start_time < duration:
                if stop_event.is_set():
                    break
                time.sleep(0.1)

            print(f"[采集进程] 录音完成，共 {frame_count} 帧")

        # 发送结束标记
        audio_queue.put(None)

    except Exception as e:
        print(f"[采集进程] 错误: {e}")
        config_queue.put({'error': str(e)})
        audio_queue.put(None)


# =============================================================================
# 主函数
# =============================================================================

def run_streaming_recognition(args):
    """运行流式识别 (multiprocessing 版本)"""

    # 创建进程间通信队列
    audio_queue = Queue(maxsize=1000)  # 限制队列大小防止内存溢出
    config_queue = Queue()
    stop_event = mp.Event()

    # 启动音频采集进程
    capture_proc = Process(
        target=audio_capture_process,
        args=(audio_queue, stop_event, args.device, args.duration, config_queue)
    )
    capture_proc.start()

    # 等待配置信息
    try:
        config = config_queue.get(timeout=10)
    except Exception:
        print("错误: 无法获取音频配置")
        capture_proc.terminate()
        return

    if 'error' in config:
        print(f"错误: {config['error']}")
        capture_proc.join()
        return

    input_rate = config['sample_rate']
    input_channels = config['channels']
    target_rate = 16000

    print(f"音频配置: {input_rate}Hz, {input_channels}ch")
    need_resample = (input_rate != target_rate)
    need_mix = (input_channels > 1)
    if need_resample:
        print(f"  -> 重采样: {input_rate}Hz → {target_rate}Hz (线性插值)")
    if need_mix:
        print(f"  -> 混音: {input_channels}ch → 1ch")

    # 配置 ASR
    lang_map = {
        'zh': evo_asr.Language.ZH,
        'en': evo_asr.Language.EN,
        'ja': evo_asr.Language.JA,
        'ko': evo_asr.Language.KO,
        'yue': evo_asr.Language.YUE,
        'auto': evo_asr.Language.AUTO,
    }

    asr_config = evo_asr.Config(args.model_dir)
    asr_config.language = lang_map[args.language]
    asr_config.punctuation_enabled = True

    print(f"ASR 配置: 语言={args.language}")
    print(f"VAD 阈值: {args.vad_threshold}")
    print(f"录制时长: {args.duration}秒")
    print("\n使用 multiprocessing 避免 GIL 导致的 overrun")
    print("说话后停顿会自动触发识别 (flush)")
    print("按 Ctrl+C 退出\n")

    # 创建回调
    callback = StreamingCallback()

    # VAD
    vad = SimpleVAD(threshold=args.vad_threshold)
    sentence_count = 0
    frame_count = 0

    with evo_asr.Engine(asr_config) as engine:
        try:
            engine.start(callback=callback)

            print("开始处理音频...")

            while True:
                try:
                    # 从队列获取音频数据 (带超时)
                    data = audio_queue.get(timeout=0.5)

                    if data is None:
                        # 结束标记
                        break

                    frame_count += 1

                    # 转换音频格式
                    audio = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0

                    if need_mix:
                        audio = stereo_to_mono(audio, input_channels)
                    if need_resample:
                        audio = resample_audio(audio, input_rate, target_rate)

                    # 发送给 ASR
                    audio_int16 = (audio * 32767).clip(-32768, 32767).astype(np.int16)
                    engine.send_audio_frame(audio_int16.tobytes())

                    # VAD 检测
                    vad.feed(audio)
                    if vad.is_sentence_end():
                        sentence_count += 1
                        print(f"[句子 {sentence_count}] flush...")
                        engine.flush()
                        vad.reset()

                    # 定期显示进度
                    if frame_count % 50 == 0:
                        print(f"[处理中] 已处理 {frame_count} 帧, 句子: {sentence_count}")

                except mp.queues.Empty:
                    # 队列超时，检查采集进程是否还在运行
                    if not capture_proc.is_alive():
                        break
                    continue

            # 处理剩余
            if vad.has_speech:
                sentence_count += 1
                print(f"[句子 {sentence_count}] flush (剩余)...")
                engine.flush()

            engine.stop()

            if callback.final_results:
                print("\n=== 识别结果汇总 ===")
                for i, text in enumerate(callback.final_results, 1):
                    print(f"  {i}. {text}")
            else:
                print("\n(无识别结果)")

        except KeyboardInterrupt:
            print("\n\n用户中断")
            stop_event.set()
            engine.stop()

    # 等待采集进程结束
    capture_proc.join(timeout=2)
    if capture_proc.is_alive():
        capture_proc.terminate()


def main():
    parser = argparse.ArgumentParser(
        description='ASR 流式识别 (VAD + flush 边录边识别, multiprocessing 版本)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
示例:
  python stream_demo.py                    # 默认设备，持续识别
  python stream_demo.py -l                 # 列出音频设备
  python stream_demo.py -d 0               # 使用设备 0
  python stream_demo.py --duration 30      # 录制 30 秒
  python stream_demo.py --language en      # 英文识别
  python stream_demo.py --vad-threshold 50000  # 降低 VAD 阈值 (更灵敏)
        """
    )
    parser.add_argument('-l', '--list', action='store_true',
                        help='列出音频设备')
    parser.add_argument('-d', '--device', type=int, default=-1,
                        help='音频设备索引 (默认: -1 自动选择)')
    parser.add_argument('--duration', type=float, default=30.0,
                        help='录音时长秒数 (默认: 30.0)')
    parser.add_argument('--model-dir', '-m', default='~/.cache/sensevoice',
                        help='模型目录 (默认: ~/.cache/sensevoice)')
    parser.add_argument('--language', default='zh',
                        choices=['zh', 'en', 'ja', 'ko', 'yue', 'auto'],
                        help='识别语言 (默认: zh)')
    parser.add_argument('--vad-threshold', type=float, default=100000.0,
                        help='VAD 能量阈值，越低越灵敏 (默认: 100000.0)')

    args = parser.parse_args()

    print(f"ASR 版本: {evo_asr.__version__}")

    if args.list:
        list_devices()
        return

    run_streaming_recognition(args)


if __name__ == "__main__":
    # multiprocessing 在某些平台需要这个
    mp.set_start_method('spawn', force=True)
    main()
